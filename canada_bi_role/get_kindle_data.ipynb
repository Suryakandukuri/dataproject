{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "from scrapy import Request, FormRequest\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class KindleBooks(scrapy.Spider):\\\n",
    "\n",
    "    name = \"kindlebooks\"\n",
    "\n",
    "    # initiating a booklist to append the data\n",
    "    book_list = []\n",
    "\n",
    "    def start_requests(self):\n",
    "        # urls for the landing page and the second page on the amazon.com new releases in last 90 days\n",
    "        start_urls = [\n",
    "            \"https://www.amazon.com/Deals-Kindle-Books-Last-90-days/s?rh=n%3A11552285011%2Cp_n_date%3A1249101011\",\n",
    "            \"https://www.amazon.com/Deals-Kindle-Books-Last-90-days/s?i=digital-text&rh=n%3A11552285011%2Cp_n_date%3A1249101011&page=2&qid=1612474374&ref=sr_pg_2\"\n",
    "        ]\n",
    "        for url in start_urls[:]:\n",
    "            yield Request(\n",
    "                url,\n",
    "                callback= self.parse\n",
    "            )\n",
    "\n",
    "\n",
    "    def parse(self, response):\n",
    "\n",
    "        # xpath value to get the names of the books and ratings\n",
    "        book_names = response.xpath('//*[@class=\"a-size-medium a-color-base a-text-normal\"]/text()').extract()\n",
    "\n",
    "        rating_data = response.xpath('//*[@class=\"a-popover-trigger a-declarative\"]/i/span/text()').extract()\n",
    "\n",
    "        # making them a dataframe\n",
    "        page_data = pd.DataFrame(\n",
    "            {\n",
    "                \"book_name\": book_names,\n",
    "                \"rating\":rating_data\n",
    "            }\n",
    "        )\n",
    "        # just to verify the data\n",
    "        print(page_data)\n",
    "        # appending it to the list\n",
    "        self.book_list.append(page_data)\n",
    "        \n",
    "        pass\n",
    "    # this works after the spider scrapes both the urls.\n",
    "    def close(self, reason):\n",
    "        # concating both the pages.\n",
    "        kindle_books = pd.concat(self.book_list)\n",
    "        kindle_books = kindle_books.reset_index(drop=True, inplace= False)\n",
    "        # adding data to csv file.\n",
    "        kindle_books.to_csv(\"kindle_ebooks_last90days.csv\", index=False)\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    This is the script to scrape data from Amazon site, to get the\n",
    "    Kindle Books that are released in the last 90 days\n",
    "    \n",
    "    Run the scraper as below.\n",
    "\n",
    "    USAGE: python get_kindle_books.py\n",
    "    \"\"\"\n",
    "    process = CrawlerProcess(\n",
    "        settings={\n",
    "            \"USER_AGENT\":\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36\",\n",
    "            \"ROBOTSTXT_OBEY\": True\n",
    "        }\n",
    "    )\n",
    "    process.crawl(KindleBooks)\n",
    "    process.start()  # the script will block here until the crawling is finished\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ]
}